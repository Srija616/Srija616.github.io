---
---
@misc{sap2021annotators,
      title={Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection},
      author={Maarten Sap and Swabha Swayamdipta and Laura Vianna and Xuhui Zhou and Yejin Choi and Noah A. Smith},
      year={2021},
      eprint={2111.07997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abbr={arXiv},
      url={https://arxiv.org/abs/2111.07997},
      abstract={The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.}
}

@misc{ethayarajh2021informationtheoretic,
    title={Information-Theoretic Measures of Dataset Difficulty},
    author={Kawin Ethayarajh and Yejin Choi and Swabha Swayamdipta},
    year={2021},
    eprint={2110.08420},
    abbr={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2110.08420},
    abstract={Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. Not only is this framework informal, but it also provides little understanding of how difficult each instance is, or what attributes make it difficult for a given model. To address these problems, we propose an information-theoretic perspective, framing dataset difficulty as the absence of usable information. Measuring usable information is as easy as measuring performance, but has certain theoretical advantages. While the latter only allows us to compare different models w.r.t the same dataset, the former also allows us to compare different datasets w.r.t the same model. We then introduce pointwise V−information (PVI) for measuring the difficulty of individual instances, where instances with higher PVI are easier for model V. By manipulating the input before measuring usable information, we can understand why a dataset is easy or difficult for a given model, which we use to discover annotation artefacts in widely-used benchmarks.}
}

@inproceedings{pillutla2021mauve,
    title={MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
    author={Krishna Pillutla and Swabha Swayamdipta and Rowan Zellers and John Thickstun and Sean Wellecks and Yejin Choi and Zaid Harchaoui},
    year={2021},
    booktitle={NeurIPS},
    abbr={NeurIPS},
    url={https://arxiv.org/abs/2102.01454},
    code={https://github.com/krishnap25/mauve},
    prize={Outstanding Paper Award},
    abstract={As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.}
}

@inproceedings{pancholy2021sister,
      title={Sister Help: Data Augmentation for Frame-Semantic Role Labeling},
      author={Ayush Pancholy and Miriam R. L. Petruck and Swabha Swayamdipta},
      year={2021},
      booktitle={LAW-DMR Workshop at EMNLP},
      abbr={EMNLP},
      url={http://arxiv.org/abs/2109.07725},
      code={https://github.com/ayush-pancholy/sister-help},
      poster={posters/sisters-ayush-lawdmr-emnlp2021.pdf},
      abstract={While FrameNet is widely regarded as a rich resource of semantics in natural language processing, a major criticism concerns its lack of coverage and the relative paucity of its labeled data compared to other commonly used lexical resources such as PropBank and VerbNet. This paper reports on a pilot study to address these gaps. We propose a data augmentation approach, which uses existing frame-specific annotation to automatically annotate other lexical units of the same frame which are unannotated. Our rule-based approach defines the notion of a sister lexical unit and generates frame-specific augmented data for training. We present experiments on frame-semantic role labeling which demonstrate the importance of this data augmentation: we obtain a large improvement to prior results on frame identification and argument identification for FrameNet, utilizing both full-text and lexicographic annotations under FrameNet. Our findings on data augmentation highlight the value of automatic resource creation for improved models in frame-semantic parsing.}
}

@inproceedings{jacovi2021contrastive,
      title={Contrastive Explanations for Model Interpretability},
      author={Alon Jacovi and Swabha Swayamdipta and Shauli Ravfogel and Yanai Elazar and Yejin Choi and Yoav Goldberg},
      year={2021},
      booktitle={EMNLP},
      abbr={EMNLP},
      url={https://arxiv.org/abs/2103.01378},
      code={https://github.com/allenai/contrastive-explanations},
      abstract={Contrastive explanations clarify why an event occurred in contrast to another. They are more inherently intuitive to humans to both produce and comprehend. We propose a methodology to produce contrastive explanations for classification models by modifying the representation to disregard non-contrastive information, and modifying model behavior to only be based on contrastive reasoning. Our method is based on projecting model representation to a latent space that captures only the features that are useful (to the model) to differentiate two potential decisions. We demonstrate the value of contrastive explanations by analyzing two different scenarios, using both high-level abstract concept attribution and low-level input token/span attribution, on two widely used text classification tasks. Specifically, we produce explanations for answering: for which label, and against which alternative label, is some aspect of the input useful? And which aspects of the input are useful for and against particular decisions? Overall, our findings shed light on the ability of label-contrastive explanations to provide a more accurate and finer-grained interpretability of a model's decision.}
}

@inproceedings{liu2021onthefly,
    title={DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
    author={Alisa Liu and Maarten Sap and Ximing Lu and Swabha Swayamdipta and Chandra Bhagavatula and Noah A. Smith and Yejin Choi},
    year={2021},
    url={https://arxiv.org/abs/2105.03023},
    booktitle={ACL},
    abbr={ACL},
    code={https://github.com/alisawuffles/DExperts},
    abstract={Despite recent advances in natural language generation, it remains challenging to control attributes of generated text. We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with "expert" LMs and/or "anti-expert" LMs in a product of experts. Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts. We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations. Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.}
}

@inproceedings{Zhou2021ToxicDebias,
            author={Xuhui Zhou and Maarten Sap and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},
            title={Challenges in Automated Debiasing for Toxic Language Detection},
            booktitle={EACL},
            abbr={EACL},
            year={2021},
            url={https://arxiv.org/abs/2102.00086},
            code={https://github.com/XuhuiZhou/Toxic_Debias},
            abstract={Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.}
}

@inproceedings{swayamdipta2020datamaps,
    title     = {Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},
    author    = {Swabha Swayamdipta and Roy Schwartz and Nicholas Lourie and Yizhong Wang and Hannaneh Hajishirzi and Noah A. Smith and Yejin Choi},
    booktitle = {EMNLP},
    abbr      = {EMNLP},
    url       = {https://arxiv.org/abs/2009.10795},
    year      = {2020},
    code      = {https://github.com/allenai/cartography},
    slides    = {https://slideslive.com/38939175},
    blog      = {1_project},
    abstract  = {Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.}
}

@inproceedings{gururangan2020dont,
    title={Don't Stop Pretraining: Adapt Language Models to Domains and Tasks},
    author={Suchin Gururangan and Ana Marasović and Swabha Swayamdipta and Kyle Lo and Iz Beltagy and Doug Downey and Noah A. Smith},
    year={2020},
    url={https://arxiv.org/abs/2004.10964},
    booktitle = {ACL},
    abbr      = {ACL},
    code  = {https://github.com/allenai/dont-stop-pretraining},
    prize = {Best Paper Honorable Mention},
    blog      = {3_project},
    abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.}
}

@inproceedings{bras2020adversarial,
    title={Adversarial Filters of Dataset Biases},
    author={Ronan LeBras and Swabha Swayamdipta and Chandra Bhagavatula and Rowan Zellers and Matthew E. Peters and Ashish Sabharwal and Yejin Choi},
    year={2020},
    booktitle={ICML},
    abbr      = {ICML},
    url={https://arxiv.org/abs/2002.04108},
    code={https://github.com/swabhs/notebooks_for_aflite},
    blog      = {2_project},
    abstract={Large neural models have demonstrated human-level performance on language and vision benchmarks, while their performance degrades considerably on adversarial or out-of-distribution samples. This raises the question of whether these models have learned to solve a dataset rather than the underlying task by overfitting to spurious dataset biases. We investigate one recently proposed approach, AFLite, which adversarially filters such dataset biases, as a means to mitigate the prevalent overestimation of machine performance. We provide a theoretical understanding for AFLite, by situating it in the generalized framework for optimum bias reduction. We present extensive supporting evidence that AFLite is broadly applicable for reduction of measurable dataset biases, and that models trained on the filtered datasets yield better generalization to out-of-distribution tasks. Finally, filtering results in a large drop in model performance (e.g., from 92% to 62% for SNLI), while human performance still remains high. Our work thus shows that such filtered datasets can pose new research challenges for robust generalization by serving as upgraded benchmarks. }
}

@inproceedings{schwartz2020right,
    title={The Right Tool for the Job: Matching Model and Instance Complexities},
    author={Roy Schwartz and Gabi Stanovsky and Swabha Swayamdipta and Jesse Dodge and Noah A. Smith},
    year={2020},
    booktitle={ACL},
    abbr      = {ACL},
    url={https://arxiv.org/abs/2004.07453},
    code={https://github.com/allenai/sledgehammer},
    abstract={As NLP models become larger, executing a trained model requires significant computational resources incurring monetary and environmental costs. To better respect a given inference budget, we propose a modification to contextual representation fine-tuning which, during inference, allows for an early (and fast) "exit" from neural network calculations for simple instances, and late (and accurate) exit for hard instances. To achieve this, we add classifiers to different layers of BERT and use their calibrated confidence scores to make early exit decisions. We test our proposed modification on five different datasets in two tasks: three text classification datasets and two natural language inference benchmarks. Our method presents a favorable speed/accuracy tradeoff in almost all cases, producing models which are up to five times faster than the state of the art, while preserving their accuracy. Our method also requires almost no additional training resources (in either time or parameters) compared to the baseline BERT model. Finally, our method alleviates the need for costly retraining of multiple models at different levels of efficiency; we allow users to control the inference speed/accuracy tradeoff using a single trained model, by setting a single variable at inference time. We publicly release our code.}
}

@inproceedings{yang2020gdaug,
    title={G-DAUG: Generative Data Augmentation for Commonsense Reasoning},
    author={Yiben Yang and Chaitanya Malaviya and Jared Fernandez and Swabha Swayamdipta and Ronan LeBras and Ji-Ping Wang and Chandra Bhagavatula and Yejin Choi and Doug Downey},
    year={2020},
    month={Jun},
    url={https://arxiv.org/abs/2004.11546},
    booktitle={EMNLP},
    abbr      = {EMNLP},
    blog      = {6_project},
    code={https://github.com/yangyiben/G-DAUG-c-Generative-Data-Augmentation-for-Commonsense-Reasoning},
    abstract={Recent advances in commonsense reasoning depend on large-scale human-annotated training data to achieve peak performance. However, manual curation of training examples is expensive and has been shown to introduce annotation artifacts that neural models can readily exploit and overfit on. We investigate G-DAUG^C, a novel generative data augmentation method that aims to achieve more accurate and robust learning in the low-resource setting. Our approach generates synthetic examples using pretrained language models, and selects the most informative and diverse set of examples for data augmentation. In experiments with multiple commonsense reasoning benchmarks, G-DAUG^C consistently outperforms existing data augmentation methods based on back-translation, and establishes a new state-of-the-art on WinoGrande, CODAH, and CommonsenseQA. Further, in addition to improvements in in-distribution accuracy, G-DAUG^C-augmented training also enhances out-of-distribution generalization, showing greater robustness against adversarial or perturbed examples. Our analysis demonstrates that G-DAUG^C produces a diverse set of fluent training examples, and that its selection and training approaches are important for performance. Our findings encourage future research toward generative data augmentation to enhance both in-distribution learning and out-of-distribution generalization.}
    }
}

---
---
@phdthesis{swayamdipta2019syntactic,
  title={{PhD Thesis: Syntactic Inductive Biases for Natural Language Processing}},
  author={Swayamdipta, Swabha},
  year={2019},
  school={Carnegie Mellon University},
  url={https://swabhs.com/assets/pdf/swabha_thesis.pdf},
  abbr      = {PhD},
  abstract = {With the rise in availability of data for language learning, the role of linguistic structure is under scrutiny. The underlying syntactic structure of language allows for composition of simple elements into more complex ones in innumerable ways; generalization to new examples hinges on this structure. We define a syntactic inductive bias as a signal that steers the learning algorithm towards a syntactically robust solution, over others. This thesis explores the need for incorporation of such biases into already powerful neural models of language. We describe three general approaches for incorporating syntactic inductive biases into task-specific models, under different levels of supervision. The first method calls for joint learning of entire syntactic dependency trees with semantic dependency graphs through direct supervision, to facilitate better semantic dependency parsing. Second, we introduce the paradigm of scaffolded learning, which enables us to leverage inductive biases from syntactic sources to predict a related semantic structure, using only as much supervision as is necessary. The third approach yields general-purpose contextualized representations conditioned on large amounts of data along with their shallow syntactic structures, obtained automatically. The linguistic representations learned as a result of syntactic inductive biases are shown to be effective across a range of downstream tasks, but their usefulness is especially pronounced for semantic tasks.}
}

@inproceedings{Ruder:19,
  title={Tutorial on Transfer Learning in Natural Language Processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={NAACL},
  abbr      = {NAACL},
  url={https://www.aclweb.org/anthology/N19-5004/},
  pages={15--18},
  year={2019},
  abstract={The classic supervised machine learning paradigm is based on learning in isolation, a single predictive model for a task using a single dataset. This approach requires a large number of training examples and performs best for well-defined and narrow tasks. Transfer learning refers to a set of methods that extend this approach by leveraging data from additional domains or tasks to train a model with better generalization properties. Over the last two years, the field of Natural Language Processing (NLP) has witnessed the emergence of several transfer learning methods and architectures which significantly improved upon the state-of-the-art on a wide range of NLP tasks. These improvements together with the wide availability and ease of integration of these methods are reminiscent of the factors that led to the success of pretrained word embeddings and ImageNet pretraining in computer vision, and indicate that these methods will likely become a common tool in the NLP landscape as well as an important research direction. We will present an overview of modern transfer learning methods in NLP, how models are pre-trained, what information the representations they learn capture, and review examples and case studies on how these models can be integrated and adapted in downstream NLP tasks.}
}

@misc{Swayamdipta:19,
    title={Shallow Syntax in Deep Water},
    author={Swabha Swayamdipta and Matthew Peters and Brendan Roof and Chris Dyer and Noah A. Smith},
    year={2019},
    eprint={1908.11047},
    archivePrefix={arXiv},
    abbr      = {arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/1908.11047},
    abstract={Shallow syntax provides an approximation of phrase-syntactic structure of sentences; it can be produced with high accuracy, and is computationally cheap to obtain. We investigate the role of shallow syntax-aware representations for NLP tasks using two techniques. First, we enhance the ELMo architecture to allow pretraining on predicted shallow syntactic parses, instead of just raw text, so that contextual embeddings make use of shallow syntactic context. Our second method involves shallow syntactic features obtained automatically on downstream task data. Neither approach leads to a significant gain on any of the four downstream tasks we considered relative to ELMo-only baselines. Further analysis using black-box probes confirms that our shallow-syntax-aware contextual embeddings do not transfer to linguistic tasks any more easily than ELMo's embeddings. We take these findings as evidence that ELMo-style pretraining discovers representations which make additional awareness of shallow syntax redundant.}
}

---
---

@inproceedings{Gururangan:18,
  title={Annotation Artifacts in Natural Language Inference Data},
  author={Gururangan, Suchin and Swayamdipta, Swabha and Levy, Omer and Schwartz, Roy and Bowman, Samuel and Smith, Noah A.},
  booktitle={NAACL},
  abbr      = {NAACL},
  year={2018},
  url={https://arxiv.org/abs/1803.02324},
  poster={posters/artifacts-naacl.pdf},
  code={https://github.com/swabhs/notebooks/blob/master/annotation_artifacts.ipynb},
  blog      = {5_project},
  abstract = {Large-scale datasets for natural language inference are created by presenting crowd workers with a sentence (premise), and asking them to generate three new sentences (hypotheses) that it entails, contradicts, or is logically neutral with respect to. We show that, in a significant portion of such data, this protocol leaves clues that make it possible to identify the label by looking only at the hypothesis, without observing the premise. Specifically, we show that a simple text categorization model can correctly classify the hypothesis alone in about 67% of SNLI (Bowman et. al, 2015) and 53% of MultiNLI (Williams et. al, 2017). Our analysis reveals that specific linguistic phenomena such as negation and vagueness are highly correlated with certain inference classes. Our findings suggest that the success of natural language inference models to date has been overestimated, and that the task remains a hard open problem.}
}

@inproceedings{Swayamdipta:18b,
  title     = {{Syntactic Scaffolds for Semantic Structures}},
  author    = {Swayamdipta, Swabha and Thomson, Sam and Lee, Kenton and Zettlemoyer, Luke and
                Dyer, Chris and Smith, Noah A.},
  booktitle = {EMNLP},
  abbr      = {EMNLP},
  year    ={2018},
  url    = {https://arxiv.org/abs/1808.10485},
  talk    = {https://youtu.be/1YK9dEjIGlU?t=40m},
  code    = {https://github.com/swabhs/scaffolding},
  blog      = {4_project},
  abstract = {We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.}
}

@inproceedings{Swayamdipta:18a,
  title={{Multi-Mention Learning for Reading Comprehension with Neural Cascades}},
  url = {https://openreview.net/forum?id=HyRnez-RW},
  author={Swayamdipta, Swabha and Parikh, Ankur P and Kwiatkowski, Tom},
  booktitle={ICLR},
  abbr      = {ICLR},
  year={2018},
  poster = {posters/triviaqa-iclr.pdf},
  abstract={Reading comprehension is a challenging task, especially when executed across longer or across multiple evidence documents, where the answer is likely to reoccur. Existing neural architectures typically do not scale to the entire evidence, and hence, resort to selecting a single passage in the document (either via truncation or other means), and carefully searching for the answer within that passage. However, in some cases, this strategy can be suboptimal,  since by focusing on a specific passage, it becomes difficult to leverage multiple mentions of the same answer throughout the document. In this work, we take a different approach by constructing lightweight models that are combined in a cascade to find the answer. Each submodel consists only of feed-forward networks equipped with an attention mechanism, making it trivially parallelizable. We show that our approach can scale to approximately an order of magnitude larger evidence documents and can aggregate information from multiple mentions of each answer candidate across the document. Empirically, our approach achieves state-of-the-art performance on both the Wikipedia and web domains of the TriviaQA dataset, outperforming more complex, recurrent architectures.}
}

@inproceedings{Peng:18a,
	title = {Learning Joint Semantic Parsers from Disjoint Data},
	author = {Peng, Hao and Thomson, Sam and Swayamdipta, Swabha and Smith, Noah A.},
	booktitle = {NAACL},
  abbr      = {NAACL},
	year={2018},
	url = {http://aclweb.org/anthology/N18-1135},
  code = {https://github.com/Noahs-ARK/NeurboParser},
  abstract={We present a new approach to learning a semantic parser from multiple datasets, even when the target semantic formalisms are drastically different and the underlying corpora do not overlap. We handle such “disjoint” data by treating annotations for unobserved formalisms as latent structured variables. Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.}
}

@inproceedings{Mulcaire:18,
  title={Polyglot Semantic Role Labeling},
  author={Mulcaire, Phoebe and Swayamdipta, Swabha and Smith, Noah A.},
  booktitle={ACL},
  abbr      = {ACL},
  year={2018},
  url={https://aclanthology.org/P18-2106/},
  abstract={Previous approaches to multilingual semantic dependency parsing treat languages independently, without exploiting the similarities between semantic structures across languages. We experiment with a new approach where we combine resources from different languages in the CoNLL 2009 shared task to build a single polyglot semantic dependency parser. Notwithstanding the absence of parallel data, and the dissimilarity in annotations between languages, our approach results in improvement in parsing performance on several languages over a monolingual baseline. Analysis of the polyglot models’ performance provides a new understanding of the similarities and differences between languages in the shared task.}
}

@inproceedings{baker-etal-2018-frame,
    title = "Frame Semantics across Languages: Towards a Multilingual {F}rame{N}et",
    author = "Baker, Collin F.  and
      Ellsworth, Michael  and
      Petruck, Miriam R. L.  and
      Swayamdipta, Swabha",
    booktitle = "COLING: Tutorial Abstracts",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-3003",
    pages = "9--12",
    abbr  = {COLING},
    abstract={FrameNet is a lexical resource that provides rich semantic representations of the core English vocabulary based on Fillmore’s Frame Semantics, with more than 200k manually annotated examples. Resources based on FrameNet have now been created for roughly a dozen languages. This workshop will present current research on aligning Frame Semantic resources across languages and automatic frame semantic parsing in English and other languages. We will explore the extent to which semantic frames are similar across languages and the implications for theories of semantic universals, the practice of translation (whether human or machine), and multilingual knowledge representation. Does not require prior familiarity with Frame Semantics.}
}
---
---


@misc{Neubig:17,
  title		= {{DyNet}: The Dynamic Neural Network Toolkit},
  author	= {Graham Neubig and Chris Dyer and Yoav Goldberg and Austin Matthews and Waleed Ammar and
              Antonios Anastasopoulos and Miguel Ballesteros and David Chiang and Daniel Clothiaux and
              Trevor Cohn and Kevin Duh and Manaal Faruqui and Cynthia Gan and Dan Garrette and
              Yangfeng Ji and Lingpeng Kong and Adhiguna Kuncoro and Gaurav Kumar and
              Chaitanya Malaviya and Paul Michel and Yusuke Oda and Matthew Richardson and
              Naomi Saphra and Swabha Swayamdipta and Pengcheng Yin},
  url    = {https://arxiv.org/abs/1701.03980},
  abbr      = {arXiv},
  year		= {2017},
  code    = {https://github.com/clab/dynet},
  abstract = {We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license.}
}

@misc{Swayamdipta:17,
  author    = {Swabha Swayamdipta and Sam Thomson and Chris Dyer and Noah A. Smith},
  title     = {{Frame-Semantic Parsing with Softmax-Margin Segmental {RNN}s and a Syntactic Scaffold}},
  url       = {http://arxiv.org/abs/1706.09528},
  year 		= {2017},
  abbr      = {arXiv},
  url    = {https://arxiv.org/abs/1706.09528},
  poster  = {posters/open-sesame.pdf},
  code    = {https://github.com/swabhs/open-sesame},
  abstract = {We present a new, efficient frame-semantic parser that labels semantic arguments to FrameNet predicates. Built using an extension to the segmental RNN that emphasizes recall, our basic system achieves competitive performance without any calls to a syntactic parser. We then introduce a method that uses phrase-syntactic annotations from the Penn Treebank during training only, through a multitask objective; no parsing is required at training or test time. This "syntactic scaffold" offers a cheaper alternative to traditional syntactic pipelining, and achieves state-of-the-art performance.}
}

---
---


@inproceedings{Swayamdipta:16,
  author    = {Swabha Swayamdipta and Miguel Ballesteros and Chris Dyer and Noah A. Smith},
  title     = {Greedy, Joint Syntactic-Semantic Parsing with {Stack LSTM}s},
  booktitle = {CoNLL},
  abbr      = {CoNLL},
  url = {https://www.aclweb.org/anthology/K16-1019},
  year      = {2016},
  slides    = {talks/conll16.pdf},
  code      = {https://github.com/swabhs/joint-lstm-parser},
  abstract  = {We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008–9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.}
}

---
---


@inproceedings{Kong:2014,
    author      = {Kong, Lingpeng and Schneider, Nathan and Swayamdipta, Swabha and Bhatia Archna
                  and Dyer, Chris and Smith, Noah A.},
    title       = {{A Dependency Parser for Tweets}},
    booktitle   = {EMNLP},
    abbr      = {EMNLP},
    year        = {2014},
    url        = {https://www.aclweb.org/anthology/D14-1108},
    code        = {https://github.com/ikekonglp/TweeboParser},
    abstract    = {We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions.}
}

@inproceedings{thomson-etal-2014-cmu,
    title = "{CMU}: Arc-Factored, Discriminative Semantic Dependency Parsing",
    author = "Thomson, Sam  and O{'}Connor, Brendan  and Flanigan, Jeffrey  and Bamman, David  and
              Dodge, Jesse  and Swayamdipta, Swabha  and Schneider, Nathan  and Dyer, Chris  and
              Smith, Noah A.",
    booktitle = "{S}em{E}val",
    abbr      = {SemEval},
    year = "2014",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S14-2027",
    doi = "10.3115/v1/S14-2027",
    pages = "176--180",
    abstract={We present an arc-factored statistical model for semantic dependency parsing, as defined by the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency Parsing. Our entry in the open track placed second in the competition.}
}

@inproceedings{matthews2014cmu,
  title={The CMU machine translation systems at WMT 2014},
  author={Matthews, Austin and Ammar, Waleed and Bhatia, Archna and Feely, Weston and Hanneman, Greg
          and Schlinger, Eva and Swayamdipta, Swabha and Tsvetkov, Yulia and Lavie, Alon and Dyer, Chris},
  booktitle={WMT},
  abbr      = {WMT},
  pages={142--149},
  year={2014},
  url={https://www.aclweb.org/anthology/W14-3315},
  abstract={We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, German–English and Hindi–English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create “synthetic translation options” that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules.}
}

---
---

@inproceedings{swayamdipta2012pursuit,
  title={{The Pursuit of Power and its Manifestation in Written Dialog}},
  author={Swayamdipta, Swabha and Rambow, Owen},
  booktitle={ICSC},
  abbr      = {ICSC},
  pages={22--29},
  year={2012},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/abstract/document/6337078},
  abstract={In this paper we explore the written dialog behavior of participants in anon line discussion for automatic identification of participants who pursue power within the discussion group. We employ various standard unsupervised machine learning approaches to make this prediction. Our approach relies on the identification of certain discourse structures and linguistic techniques used by participants in the discussion. We achive an F-measure of 69.5% using unsupervised methods.}
}
